{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souissiaya/Facial-Emotions-Detection/blob/main/FacialEmotionsDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC_tWHBV4XjX"
      },
      "outputs": [],
      "source": [
        "#Install libraries\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H974Ai3G4nvK"
      },
      "outputs": [],
      "source": [
        "#Install library\n",
        "!pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz7fomE94ivD"
      },
      "outputs": [],
      "source": [
        "#Import Required Packages\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from keras_preprocessing.image import load_img, img_to_array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwke-etL4w2y"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYfs47DX45D6"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "#copy the kaggle.jsom to the folder created\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "#Permission for the json to act\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d msambare/fer2013"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyAO69DG48kS"
      },
      "outputs": [],
      "source": [
        "!unzip fer2013.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNJVW2ZRoOiS"
      },
      "source": [
        "**Image count for each class**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUCK8BpG4u2J"
      },
      "outputs": [],
      "source": [
        "# Define name of each classes (all image folder names)\n",
        "classes = ['angry', 'disgust', 'fear', 'happy', 'neutral','sad', 'surprise']\n",
        "\n",
        "# Print number of images for each class\n",
        "folder_path = \"data/\"\n",
        "for cls in classes:\n",
        "    path = os.path.join(folder_path, '/content/train', cls)\n",
        "    lst = os.listdir(path)\n",
        "    number_files = len(lst)\n",
        "    print(cls, ': ', number_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHHIBufhobK4"
      },
      "source": [
        "**Create a folder structure using Python**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVJl-jtV5DQa"
      },
      "outputs": [],
      "source": [
        "# Create a empty folder\n",
        "os.makedirs('new_data')\n",
        "# Create \"train\" folder inside new_data folder\n",
        "os.makedirs('new_data/train')\n",
        "# Crate sub-folders\n",
        "for cls in classes:\n",
        "    os.makedirs('new_data/train/'+cls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_qLM8HCogOg"
      },
      "source": [
        "**Copy images to Subfolder**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuNvzLJQ5LIq"
      },
      "outputs": [],
      "source": [
        "# Copy 436 files to new folder\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "num_files = 436\n",
        "\n",
        "for cls in classes:\n",
        "    # Downloaded original training image folder path for face emotion recognition\n",
        "    src_path = os.path.join('/content', 'train', cls)\n",
        "    # Sub folder path\n",
        "    dst_path = os.path.join('/content/new_data', 'train', cls)\n",
        "    src_files = os.listdir(src_path)\n",
        "    # Select random 436 images from source directory\n",
        "    src_select_files = random.sample(src_files, num_files)\n",
        "\n",
        "    # Copy selected images to destination folder\n",
        "    for file_name in src_select_files:\n",
        "        full_file_name = os.path.join(src_path, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, dst_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_files = 436\n",
        "\n",
        "for cls in classes:\n",
        "    # Downloaded original training image folder path for face emotion recognition\n",
        "    src_path = os.path.join('/content', 'train', cls)\n",
        "    # Sub folder path\n",
        "    dst_path = os.path.join('/content/new_data', 'train', cls)\n",
        "    src_files = os.listdir(src_path)\n",
        "    # Select random 436 images from source directory\n",
        "    src_select_files = random.sample(src_files, num_files)\n",
        "\n",
        "    # Copy selected images to destination folder\n",
        "    for file_name in src_select_files:\n",
        "        full_file_name = os.path.join(src_path, file_name)\n",
        "        if os.path.isfile(full_file_name):\n",
        "            shutil.copy(full_file_name, dst_path)\n"
      ],
      "metadata": {
        "id": "hlFyRCoeWV1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feQHGm0s5Nqq"
      },
      "outputs": [],
      "source": [
        "# Print number of images for each class\n",
        "folder_path = \"new_data/\"\n",
        "for cls in classes:\n",
        "    path = os.path.join(folder_path, 'train', cls)\n",
        "    lst = os.listdir(path)\n",
        "    number_files = len(lst)\n",
        "    print(cls, ': ', number_files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POkRAUyPonFp"
      },
      "source": [
        "**Show images using OpenCV**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOc0clAl5aL7"
      },
      "outputs": [],
      "source": [
        "picture_size = 48\n",
        "folder_path = \"new_data/\"\n",
        "\n",
        "expression = 'disgust'\n",
        "\n",
        "plt.figure(figsize= (12,12))\n",
        "for i in range(1, 10, 1):\n",
        "    plt.subplot(3,3,i)\n",
        "    img = load_img(folder_path+\"train/\"+expression+\"/\"+\n",
        "                  os.listdir(folder_path + \"train/\" + expression)[i], target_size=(picture_size, picture_size))\n",
        "    plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXZO_B2AorIR"
      },
      "source": [
        "**Change Image size**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J8ipnDx5eHq"
      },
      "outputs": [],
      "source": [
        "# Function to Read all the images: resize and convert in them to array using opencv\n",
        "\n",
        "img_size = 224 ## ImageNet => 224x224\n",
        "training_data = []\n",
        "\n",
        "for category in classes:\n",
        "  path = os.path.join(folder_path, 'train', category)\n",
        "  class_num = classes.index(category)\n",
        "  for img in os.listdir(path):\n",
        "      try:\n",
        "          img_array = cv2.imread(os.path.join(path, img))\n",
        "          new_array = cv2.resize(img_array, (img_size, img_size))\n",
        "          training_data.append([new_array, class_num])\n",
        "      except Exception as e:\n",
        "          pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtqnKBVI5jya"
      },
      "outputs": [],
      "source": [
        "# Check the training data shape\n",
        "temp_array = np.array(training_data)\n",
        "temp_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALvUTELG6Skj"
      },
      "outputs": [],
      "source": [
        "# The shape of one converted image\n",
        "# Reading one image from angry folder\n",
        "img_array = cv2.imread('/content/new_data/train/disgust/Training_11660541.jpg')\n",
        "print('Input image shape: ', img_array.shape)\n",
        "\n",
        "# Convert image to 224x224\n",
        "img_size = 224 ## ImageNet => 224x224\n",
        "new_array = cv2.resize(img_array, (img_size, img_size))\n",
        "print('Converted image shape: ', new_array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Fv1Z21pBGc"
      },
      "source": [
        "**Add image dimensions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iacyfw5G6cd0"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features, label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, img_size, img_size, 3) # converting it to 4 dimention\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "# Convert to array\n",
        "Y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7cwdiK-pGix"
      },
      "source": [
        "**Setup MobileNet model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzSRqVgJ6iKb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Download Pre-trained MobileNet Model\n",
        "model = tf.keras.applications.MobileNetV2() ## Pre-trained Model\n",
        "\n",
        "# Print MobileNet architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtQq1spjpagJ"
      },
      "source": [
        "**Note:** In the model architecture, if you see the last layer has 1000 classes. But for our case, we have only 7 classes (angry, disgust, fear, happy, neutral, sad, and surprise). So we need to change the last layer of the downloaded MobileNet pre-trained model. This technique is called Transfer Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uc066BXb6pTZ"
      },
      "outputs": [],
      "source": [
        "# Defining first layer as input layer of Mobilenet\n",
        "base_input = model.layers[0].input\n",
        "print(base_input)\n",
        "\n",
        "# Removing last layer of MobileNet model\n",
        "base_output = model.layers[-2].output\n",
        "\n",
        "# Adding some extra layers\n",
        "final_output = layers.Dense(128)(base_output) ## adding new layer, after the output of global pooling layer\n",
        "final_output = layers.Activation('relu')(final_output) ## activation function\n",
        "final_output = layers.Dense(64)(final_output)\n",
        "final_output = layers.Activation('relu')(final_output)\n",
        "# Defining final layer with 7 classes\n",
        "final_output = layers.Dense(7, activation = 'softmax')(final_output) ## 7 because my classes are 7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seLdTOlNp16J"
      },
      "source": [
        "**Note:** Here in this code we are removing the last layer from the downloaded model and adding our customized output layer with some additional layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf9fyVfN6sbz"
      },
      "outputs": [],
      "source": [
        "custom_model = keras.Model(inputs = base_input, outputs = final_output) ## Final model architecture\n",
        "# Print our custom model summary\n",
        "custom_model.summary()\n",
        "\n",
        "# Compiling the model to train\n",
        "custom_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhPBmcHrqSLJ"
      },
      "source": [
        "For this example, I am using:\n",
        "  - Sparse categorical cross entropy as loss function\n",
        "  - adam as optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6Dk0-KIqe7J"
      },
      "source": [
        "Now, I am going to use batch size = 8 and 25 epochs to avoid any memory error while training in the local windows system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asFDR8_I6wtj"
      },
      "outputs": [],
      "source": [
        "custom_model.fit(X, Y, epochs = 80, batch_size = 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7yKe3hDrAYx"
      },
      "source": [
        "The final accuracy of our custom emotion detection model is 98%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elixn-1VrDMJ"
      },
      "source": [
        "**Save trained model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5vZ9J-OrFux"
      },
      "outputs": [],
      "source": [
        "custom_model.save('facial_expression_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-BHnJczrKMJ"
      },
      "source": [
        "We are done with our custom model training for emotion detection. Now letâ€™s test how our model is performing for any image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UxWjbZQrJkh"
      },
      "outputs": [],
      "source": [
        "# Read downloaded test image in Opencv\n",
        "test_img = cv2.imread('/content/test/neutral/PrivateTest_11752870.jpg')\n",
        "# Take a backup of input image before face detection\n",
        "img_bcp = test_img.copy()\n",
        "\n",
        "# Show image in OpenCV\n",
        "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOyJBzXNrodj"
      },
      "source": [
        "**Face detection and cropping**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmyBNHL3rzGD"
      },
      "source": [
        "To detect facial expressions we first need to detect the face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZNH_KPhrn3r"
      },
      "outputs": [],
      "source": [
        "#Define haar cascade classifier for face detection\n",
        "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Convert image to gray scale OpenCV\n",
        "gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect face using haar cascade classifier\n",
        "faces_coordinates = face_classifier.detectMultiScale(gray_img)\n",
        "\n",
        "# Draw a rectangle around the faces\n",
        "for (x, y, w, h) in faces_coordinates:\n",
        "    # Draw rectangle around face\n",
        "    cv2.rectangle(test_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Crop face from image\n",
        "    cropped_face = img_bcp[y:y+h, x:x+w]\n",
        "\n",
        "# Plot original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Plot cropped image after performing face detection\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8gfLsm1Ln_J"
      },
      "source": [
        "**Model Prediction**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvXh_g-ELdIz"
      },
      "outputs": [],
      "source": [
        "#  Creating class dictionary\n",
        "class_dictionary = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
        "\n",
        "final_image = cv2.resize(test_img, (224,224))\n",
        "final_image = np.expand_dims(final_image, axis=0) ## Need 4th dimension\n",
        "final_image = final_image/255.0 ## Normalizing\n",
        "\n",
        "# Load model\n",
        "new_model = tf.keras.models.load_model('/content/facial_expression_model.h5')\n",
        "prediction = new_model.predict(final_image)\n",
        "class_dictionary[np.argmax(prediction)]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1a_1qZUTKCaSaSmbbIeaqnIZs0AkJWeMZ",
      "authorship_tag": "ABX9TyMdni7NPcLyX6rrQ5bJ5nI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}